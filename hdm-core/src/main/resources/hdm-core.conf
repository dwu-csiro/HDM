hdm {
  cluster {
    master.url = "${master.url}"
  }
  callback.worker {
    team-size = 16
  }
  executor{
    parallelism.factor = 2.0
    mockExecution = false
  }
  planner{
    parallelism {
      # affect the number of parallel tasks planned for each physical node, default = number of cores
      cpu.factor = 4
      # affect the number of shuffle tasks planned for each physical node, default = 1/2 * number of cores
      network.factor = 4
    }
    input {
      # whether the input will be grouped according to parallelism and location
      group = false
      # the group policy implementation, options: boundary, weighted, minmin
      group-policy = "weighted"
    }
  }
  io.netty {
    server{
      port = "9091"
      threads = 16
    }
    client {
      threads = 8
      connection-per-peer = 1
    }
  }
  io.network.block.compress = false
  scheduling.policy {
    # must be classes which implemented the interface: org.hdm.core.scheduling.SchedulingPolicy
    class = "org.hdm.core.scheduling.MinminSchedulingOpt"

    # factors used to estimate the time cost for processing one unit of data on different types of resources
    # the smaller of the value, the faster the resource is
    factor {
      cpu = 1
      io = 5
      network = 10
    }
  }
}

callback-dispatcher {
  type = BalancingDispatcher
  executor = "fork-join-executor"
  fork-join-executor {
    # Min number of threads to cap factor-based parallelism number to
    parallelism-min = 8

    # Parallelism (threads) ... ceil(available processors * factor)
    parallelism-factor = 1.0

    # Max number of threads to cap factor-based parallelism number to
    parallelism-max = 64
  }

}

http-dispatcher {
  type = BalancingDispatcher
  executor = "fork-join-executor"
  fork-join-executor {
    # Min number of threads to cap factor-based parallelism number to
    parallelism-min = 8

    # Parallelism (threads) ... ceil(available processors * factor)
    parallelism-factor = 1.0

    # Max number of threads to cap factor-based parallelism number to
    parallelism-max = 64
  }

}

akka{

  #loggers = ["akka.event.slf4j.Slf4jLogger"]

  # Options: OFF, ERROR, WARNING, INFO, DEBUG
  #loglevel = "INFO"

  #stdout-loglevel = "INFO"

  actor {
    provider = "akka.remote.RemoteActorRefProvider"
    serialize-messages = off
    serializers {
      java = "akka.serialization.JavaSerializer"
      akka = "akka.serialization.JavaSerializer"
      proto = "akka.remote.serialization.ProtobufSerializer"
      bytes = "akka.serialization.ByteArraySerializer"
      hdm = "org.hdm.core.serializer.DynamicAkkaSerializer"
    }

    serialization-bindings {
      "java.nio.ByteBuffer" = none
      "java.io.Serializable" = hdm
      "java.lang.Object" = hdm
      "scala.collection.immutable.List" = hdm
      "org.hdm.core.functions.SerializableFunction" = hdm
      "org.hdm.core.message.SchedulingMsg" = hdm
    }
  }

  io {
    tcp {
      # When trying to assign a new connection to a selector and the chosen
      # selector is at full capacity, retry selector choosing and assignment
      # this many times before giving up
      selector-association-retries = 10

      # The maximum number of connection that are accepted in one go,
      # higher numbers decrease latency, lower numbers increase fairness on
      # the worker-dispatcher
      batch-accept-limit = 10
    }
  }

  remote {
    enabled-transports = ["akka.remote.netty.tcp"]
    netty.tcp {
      hostname = ""
      port= "8999"
      # Controls the backoff interval after a refused write is reattempted.
      # (Transports may refuse writes if their internal buffer is full)
      backoff-interval = 10 ms

      # Acknowledgment timeout of management commands sent to the transport stack.
      command-ack-timeout = 300 s

      # Sets the send buffer size of the Sockets,
      # set to 0b for platform default
      send-buffer-size = 256000000b

      # Sets the receive buffer size of the Sockets,
      # set to 0b for platform default
      receive-buffer-size = 256000000b

      # Maximum message size the transport will accept, but at least
      # 32000 bytes.
      # Please note that UDP does not support arbitrary large datagrams,
      # so this setting has to be chosen carefully when using UDP.
      # Both $send-buffer-size and $receive-buffer-size settings has to
      # be adjusted to be able to buffer messages of maximum size.
      maximum-frame-size = 128000000b

      # Used to configure the number of I/O worker threads on server sockets
      server-socket-worker-pool {
        # Min number of threads to cap factor-based number to
        pool-size-min = 2

        # The pool size factor is used to determine thread pool size
        # using the following formula: ceil(available processors * factor).
        # Resulting size is then bounded by the pool-size-min and
        # pool-size-max values.
        pool-size-factor = 1.0

        # Max number of threads to cap factor-based number to
        pool-size-max = 2
      }

      # Used to configure the number of I/O worker threads on client sockets
      client-socket-worker-pool {
        # Min number of threads to cap factor-based number to
        pool-size-min = 2

        # The pool size factor is used to determine thread pool size
        # using the following formula: ceil(available processors * factor).
        # Resulting size is then bounded by the pool-size-min and
        # pool-size-max values.
        pool-size-factor = 1.0

        # Max number of threads to cap factor-based number to
        pool-size-max = 2
      }
    }
    # If this is "on", Akka will log all inbound messages at DEBUG level,
    # if off then they are not logged
    log-received-messages = off

    # If this is "on", Akka will log all outbound messages at DEBUG level,
    # if off then they are not logged
    log-sent-messages = off
  }
}
